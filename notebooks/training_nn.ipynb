{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "from training.Helpers import metrics,make_model_nn,load_nn_data,take_params,test_nn_model\n",
    "from tqdm import tqdm\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos =  {\n",
    "    'dense1':[32,64,173,128,256],\n",
    "    'dense2':[64,128,256,512],\n",
    "    'drop2':[0.1,0],\n",
    "    'dense3':[64,128,256,512,1024],\n",
    "    'drop3':[0.1,0,0.2],\n",
    "    'dense4':[64,128,256,512,1024],\n",
    "    'drop4':[0.1,0,0.2],\n",
    "    'dense5':[64,128,256],\n",
    "    'dense6':[64,128,256],\n",
    "    'drop6':[0.1,0,0.2],\n",
    "    'bn':[0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_params = take_params(pos,1000)\n",
    "pd.DataFrame(diff_params).to_pickle(f'../results/best_hist_nn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [07:01, 421.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22344/1423101730.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_nn_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_model_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         h = model.fit(X_train, y_train, batch_size=128, epochs=150, validation_data=(X_val, y_val), \n\u001b[0m\u001b[0;32m      9\u001b[0m             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True,start_from_epoch=30)],verbose =0)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "best_acc_30k = -1\n",
    "for id,params in tqdm(enumerate(diff_params)):\n",
    "    history = []\n",
    "    for sample_size in [1000,2000,5000,8000,10000,15000,20000,30000]:\n",
    "        X_train,X_val,y_train,y_val = load_nn_data(sample_size)\n",
    "        model = make_model_nn(params)\n",
    "        h = model.fit(X_train, y_train, batch_size=128, epochs=150, validation_data=(X_val, y_val), \n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True,start_from_epoch=30)],verbose =0)\n",
    "        \n",
    "        h2  = {\n",
    "            'val_loss':h.history['val_loss'],\n",
    "            'val_accuracy':h.history['val_accuracy'],\n",
    "            'loss':h.history['loss'],\n",
    "            'accuracy':h.history['accuracy'],\n",
    "            'sample_size' : sample_size,\n",
    "        }\n",
    "        \n",
    "        history.append(h2)\n",
    "        \n",
    "        res = test_nn_model(model)\n",
    "        res['sample_size'] = sample_size\n",
    "        res['params_id'] = id\n",
    "        results.append(res)\n",
    "    \n",
    "    pd.DataFrame(results).to_pickle(f'../results/res_nn_.pkl')\n",
    "    if res['accuracy_score'] > best_acc_30k:\n",
    "        best_acc_30k = res['accuracy_score']\n",
    "        pd.DataFrame(history).to_pickle(f'../results/best_hist_nn.pkl')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6/6 [==============================] - 2s 45ms/step - loss: 577.2158 - accuracy: 0.0071 - val_loss: 130.9742 - val_accuracy: 0.0033\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 199.4464 - accuracy: 0.0100 - val_loss: 39.5973 - val_accuracy: 0.0100\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 108.9437 - accuracy: 0.0186 - val_loss: 20.3671 - val_accuracy: 0.0133\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 71.5282 - accuracy: 0.0100 - val_loss: 13.2498 - val_accuracy: 0.0066\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 50.0059 - accuracy: 0.0086 - val_loss: 9.4296 - val_accuracy: 0.0100\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.3791 - accuracy: 0.0114 - val_loss: 6.1746 - val_accuracy: 0.0100\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 29.6024 - accuracy: 0.0143 - val_loss: 5.1851 - val_accuracy: 0.0266\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 25.4653 - accuracy: 0.0071 - val_loss: 4.9476 - val_accuracy: 0.0133\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 21.6647 - accuracy: 0.0143 - val_loss: 4.8689 - val_accuracy: 0.0066\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 19.3612 - accuracy: 0.0143 - val_loss: 4.7344 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.5334 - accuracy: 0.0114 - val_loss: 4.7034 - val_accuracy: 0.0133\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 15.5833 - accuracy: 0.0171 - val_loss: 4.6787 - val_accuracy: 0.0100\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 13.4604 - accuracy: 0.0043 - val_loss: 4.6418 - val_accuracy: 0.0199\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12.0019 - accuracy: 0.0043 - val_loss: 4.6353 - val_accuracy: 0.0266\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.7518 - accuracy: 0.0129 - val_loss: 4.6243 - val_accuracy: 0.0133\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.8440 - accuracy: 0.0129 - val_loss: 4.6173 - val_accuracy: 0.0166\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7279 - accuracy: 0.0100 - val_loss: 4.6120 - val_accuracy: 0.0100\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.0011 - accuracy: 0.0100 - val_loss: 4.6104 - val_accuracy: 0.0066\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.7441 - accuracy: 0.0086 - val_loss: 4.6091 - val_accuracy: 0.0066\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.1535 - accuracy: 0.0057 - val_loss: 4.6083 - val_accuracy: 0.0066\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.5892 - accuracy: 0.0129 - val_loss: 4.6076 - val_accuracy: 0.0100\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.1508 - accuracy: 0.0129 - val_loss: 4.6073 - val_accuracy: 0.0100\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0476 - accuracy: 0.0100 - val_loss: 4.6072 - val_accuracy: 0.0100\n",
      "Epoch 24/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8814 - accuracy: 0.0143 - val_loss: 4.6070 - val_accuracy: 0.0100\n",
      "Epoch 25/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5864 - accuracy: 0.0186 - val_loss: 4.6068 - val_accuracy: 0.0100\n",
      "Epoch 26/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5928 - accuracy: 0.0100 - val_loss: 4.6067 - val_accuracy: 0.0100\n",
      "Epoch 27/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4792 - accuracy: 0.0143 - val_loss: 4.6065 - val_accuracy: 0.0100\n",
      "Epoch 28/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.3359 - accuracy: 0.0186 - val_loss: 4.6064 - val_accuracy: 0.0100\n",
      "Epoch 29/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.3643 - accuracy: 0.0114 - val_loss: 4.6062 - val_accuracy: 0.0100\n",
      "Epoch 30/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4706 - accuracy: 0.0129 - val_loss: 4.6060 - val_accuracy: 0.0100\n",
      "Epoch 31/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.3903 - accuracy: 0.0143 - val_loss: 4.6058 - val_accuracy: 0.0100\n",
      "Epoch 32/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.3318 - accuracy: 0.0114 - val_loss: 4.6056 - val_accuracy: 0.0100\n",
      "Epoch 33/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.1721 - accuracy: 0.0114 - val_loss: 4.6054 - val_accuracy: 0.0100\n",
      "Epoch 34/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.1190 - accuracy: 0.0114 - val_loss: 4.6052 - val_accuracy: 0.0100\n",
      "Epoch 35/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9921 - accuracy: 0.0143 - val_loss: 4.6050 - val_accuracy: 0.0100\n",
      "Epoch 36/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.0114 - accuracy: 0.0100 - val_loss: 4.6049 - val_accuracy: 0.0100\n",
      "Epoch 37/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.1742 - accuracy: 0.0157 - val_loss: 4.6047 - val_accuracy: 0.0100\n",
      "Epoch 38/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.0975 - accuracy: 0.0157 - val_loss: 4.6045 - val_accuracy: 0.0100\n",
      "Epoch 39/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9237 - accuracy: 0.0200 - val_loss: 4.6043 - val_accuracy: 0.0199\n",
      "Epoch 40/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9886 - accuracy: 0.0143 - val_loss: 4.6041 - val_accuracy: 0.0199\n",
      "Epoch 41/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9128 - accuracy: 0.0186 - val_loss: 4.6039 - val_accuracy: 0.0066\n",
      "Epoch 42/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.8814 - accuracy: 0.0129 - val_loss: 4.6037 - val_accuracy: 0.0066\n",
      "Epoch 43/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.0237 - accuracy: 0.0114 - val_loss: 4.6035 - val_accuracy: 0.0066\n",
      "Epoch 44/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.8777 - accuracy: 0.0129 - val_loss: 4.6033 - val_accuracy: 0.0066\n",
      "Epoch 45/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9385 - accuracy: 0.0129 - val_loss: 4.6031 - val_accuracy: 0.0066\n",
      "Epoch 46/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.8353 - accuracy: 0.0100 - val_loss: 4.6030 - val_accuracy: 0.0066\n",
      "Epoch 47/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9066 - accuracy: 0.0157 - val_loss: 4.6028 - val_accuracy: 0.0066\n",
      "Epoch 48/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.8459 - accuracy: 0.0157 - val_loss: 4.6027 - val_accuracy: 0.0066\n",
      "Epoch 49/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7415 - accuracy: 0.0157 - val_loss: 4.6025 - val_accuracy: 0.0066\n",
      "Epoch 50/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7847 - accuracy: 0.0157 - val_loss: 4.6023 - val_accuracy: 0.0066\n",
      "Epoch 51/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.8466 - accuracy: 0.0186 - val_loss: 4.6022 - val_accuracy: 0.0066\n",
      "Epoch 52/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7977 - accuracy: 0.0157 - val_loss: 4.6021 - val_accuracy: 0.0066\n",
      "Epoch 53/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.7303 - accuracy: 0.0129 - val_loss: 4.6019 - val_accuracy: 0.0066\n",
      "Epoch 54/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.8017 - accuracy: 0.0086 - val_loss: 4.6017 - val_accuracy: 0.0199\n",
      "Epoch 55/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7603 - accuracy: 0.0171 - val_loss: 4.6015 - val_accuracy: 0.0199\n",
      "Epoch 56/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6897 - accuracy: 0.0086 - val_loss: 4.6014 - val_accuracy: 0.0066\n",
      "Epoch 57/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7804 - accuracy: 0.0186 - val_loss: 4.6013 - val_accuracy: 0.0199\n",
      "Epoch 58/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6627 - accuracy: 0.0157 - val_loss: 4.6012 - val_accuracy: 0.0199\n",
      "Epoch 59/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6868 - accuracy: 0.0143 - val_loss: 4.6011 - val_accuracy: 0.0199\n",
      "Epoch 60/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7155 - accuracy: 0.0171 - val_loss: 4.6010 - val_accuracy: 0.0199\n",
      "Epoch 61/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6731 - accuracy: 0.0157 - val_loss: 4.6009 - val_accuracy: 0.0199\n",
      "Epoch 62/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6861 - accuracy: 0.0200 - val_loss: 4.6008 - val_accuracy: 0.0199\n",
      "Epoch 63/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7363 - accuracy: 0.0157 - val_loss: 4.6007 - val_accuracy: 0.0199\n",
      "Epoch 64/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6874 - accuracy: 0.0129 - val_loss: 4.6005 - val_accuracy: 0.0199\n",
      "Epoch 65/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.7921 - accuracy: 0.0157 - val_loss: 4.6004 - val_accuracy: 0.0199\n",
      "Epoch 66/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.7038 - accuracy: 0.0171 - val_loss: 4.6003 - val_accuracy: 0.0199\n",
      "Epoch 67/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6963 - accuracy: 0.0157 - val_loss: 4.6002 - val_accuracy: 0.0199\n",
      "Epoch 68/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7143 - accuracy: 0.0157 - val_loss: 4.6001 - val_accuracy: 0.0199\n",
      "Epoch 69/150\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.7404 - accuracy: 0.0200 - val_loss: 4.6001 - val_accuracy: 0.0199\n",
      "Epoch 70/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.6388 - accuracy: 0.0200 - val_loss: 4.6000 - val_accuracy: 0.0199\n",
      "Epoch 71/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.6348 - accuracy: 0.0143 - val_loss: 4.5999 - val_accuracy: 0.0199\n",
      "Epoch 72/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.6358 - accuracy: 0.0157 - val_loss: 4.5997 - val_accuracy: 0.0199\n",
      "Epoch 73/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7456 - accuracy: 0.0171 - val_loss: 4.5996 - val_accuracy: 0.0199\n",
      "Epoch 74/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6547 - accuracy: 0.0157 - val_loss: 4.5995 - val_accuracy: 0.0199\n",
      "Epoch 75/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6470 - accuracy: 0.0157 - val_loss: 4.5994 - val_accuracy: 0.0199\n",
      "Epoch 76/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6943 - accuracy: 0.0114 - val_loss: 4.5992 - val_accuracy: 0.0233\n",
      "Epoch 77/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6555 - accuracy: 0.0214 - val_loss: 4.5991 - val_accuracy: 0.0233\n",
      "Epoch 78/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.6255 - accuracy: 0.0186 - val_loss: 4.5990 - val_accuracy: 0.0233\n",
      "Epoch 79/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.6533 - accuracy: 0.0214 - val_loss: 4.5989 - val_accuracy: 0.0233\n",
      "Epoch 80/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.6267 - accuracy: 0.0186 - val_loss: 4.5988 - val_accuracy: 0.0233\n",
      "Epoch 81/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6572 - accuracy: 0.0171 - val_loss: 4.5988 - val_accuracy: 0.0233\n",
      "Epoch 82/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5830 - accuracy: 0.0186 - val_loss: 4.5987 - val_accuracy: 0.0233\n",
      "Epoch 83/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6654 - accuracy: 0.0186 - val_loss: 4.5987 - val_accuracy: 0.0233\n",
      "Epoch 84/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6184 - accuracy: 0.0186 - val_loss: 4.5987 - val_accuracy: 0.0233\n",
      "Epoch 85/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6777 - accuracy: 0.0171 - val_loss: 4.5986 - val_accuracy: 0.0233\n",
      "Epoch 86/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6324 - accuracy: 0.0186 - val_loss: 4.5986 - val_accuracy: 0.0233\n",
      "Epoch 87/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6412 - accuracy: 0.0171 - val_loss: 4.5985 - val_accuracy: 0.0233\n",
      "Epoch 88/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6062 - accuracy: 0.0200 - val_loss: 4.5984 - val_accuracy: 0.0233\n",
      "Epoch 89/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5884 - accuracy: 0.0157 - val_loss: 4.5983 - val_accuracy: 0.0233\n",
      "Epoch 90/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5996 - accuracy: 0.0200 - val_loss: 4.5983 - val_accuracy: 0.0233\n",
      "Epoch 91/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6359 - accuracy: 0.0186 - val_loss: 4.5983 - val_accuracy: 0.0233\n",
      "Epoch 92/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6301 - accuracy: 0.0186 - val_loss: 4.5983 - val_accuracy: 0.0233\n",
      "Epoch 93/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.6424 - accuracy: 0.0186 - val_loss: 4.5982 - val_accuracy: 0.0233\n",
      "Epoch 94/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.5947 - accuracy: 0.0171 - val_loss: 4.5981 - val_accuracy: 0.0233\n",
      "Epoch 95/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.6032 - accuracy: 0.0186 - val_loss: 4.5981 - val_accuracy: 0.0233\n",
      "Epoch 96/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.6148 - accuracy: 0.0171 - val_loss: 4.5980 - val_accuracy: 0.0233\n",
      "Epoch 97/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.5914 - accuracy: 0.0200 - val_loss: 4.5980 - val_accuracy: 0.0233\n",
      "Epoch 98/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.5682 - accuracy: 0.0171 - val_loss: 4.5979 - val_accuracy: 0.0233\n",
      "Epoch 99/150\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 4.5767 - accuracy: 0.0171 - val_loss: 4.5979 - val_accuracy: 0.0233\n",
      "Epoch 100/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6452 - accuracy: 0.0171 - val_loss: 4.5979 - val_accuracy: 0.0233\n",
      "Epoch 101/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5899 - accuracy: 0.0186 - val_loss: 4.5979 - val_accuracy: 0.0233\n",
      "Epoch 102/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.5862 - accuracy: 0.0171 - val_loss: 4.5979 - val_accuracy: 0.0233\n",
      "Epoch 103/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5824 - accuracy: 0.0200 - val_loss: 4.5978 - val_accuracy: 0.0233\n",
      "Epoch 104/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5997 - accuracy: 0.0186 - val_loss: 4.5978 - val_accuracy: 0.0233\n",
      "Epoch 105/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5907 - accuracy: 0.0214 - val_loss: 4.5978 - val_accuracy: 0.0233\n",
      "Epoch 106/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5810 - accuracy: 0.0186 - val_loss: 4.5978 - val_accuracy: 0.0233\n",
      "Epoch 107/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5855 - accuracy: 0.0186 - val_loss: 4.5978 - val_accuracy: 0.0233\n",
      "Epoch 108/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6327 - accuracy: 0.0186 - val_loss: 4.5978 - val_accuracy: 0.0233\n",
      "Epoch 109/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5946 - accuracy: 0.0157 - val_loss: 4.5977 - val_accuracy: 0.0233\n",
      "Epoch 110/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5974 - accuracy: 0.0171 - val_loss: 4.5977 - val_accuracy: 0.0233\n",
      "Epoch 111/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6357 - accuracy: 0.0186 - val_loss: 4.5976 - val_accuracy: 0.0233\n",
      "Epoch 112/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5889 - accuracy: 0.0186 - val_loss: 4.5976 - val_accuracy: 0.0233\n",
      "Epoch 113/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5566 - accuracy: 0.0186 - val_loss: 4.5976 - val_accuracy: 0.0233\n",
      "Epoch 114/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6011 - accuracy: 0.0186 - val_loss: 4.5976 - val_accuracy: 0.0233\n",
      "Epoch 115/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5688 - accuracy: 0.0186 - val_loss: 4.5976 - val_accuracy: 0.0233\n",
      "Epoch 116/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6187 - accuracy: 0.0186 - val_loss: 4.5977 - val_accuracy: 0.0233\n",
      "Epoch 117/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5534 - accuracy: 0.0214 - val_loss: 4.5976 - val_accuracy: 0.0233\n",
      "Epoch 118/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5849 - accuracy: 0.0186 - val_loss: 4.5977 - val_accuracy: 0.0233\n",
      "Epoch 119/150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.5677 - accuracy: 0.0186 - val_loss: 4.5977 - val_accuracy: 0.0233\n",
      "312/312 [==============================] - 0s 850us/step\n",
      "Epoch 1/150\n",
      "11/11 [==============================] - 1s 24ms/step - loss: 334.6587 - accuracy: 0.0064 - val_loss: 39.1230 - val_accuracy: 0.0033\n",
      "Epoch 2/150\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 83.3530 - accuracy: 0.0086 - val_loss: 14.7909 - val_accuracy: 0.0133\n",
      "Epoch 3/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.8661 - accuracy: 0.0171 - val_loss: 5.4268 - val_accuracy: 0.0117\n",
      "Epoch 4/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 24.8605 - accuracy: 0.0079 - val_loss: 5.0326 - val_accuracy: 0.0133\n",
      "Epoch 5/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 18.2969 - accuracy: 0.0064 - val_loss: 4.6649 - val_accuracy: 0.0150\n",
      "Epoch 6/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 13.9115 - accuracy: 0.0100 - val_loss: 4.7864 - val_accuracy: 0.0050\n",
      "Epoch 7/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 11.3222 - accuracy: 0.0143 - val_loss: 4.8428 - val_accuracy: 0.0033\n",
      "Epoch 8/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 8.9600 - accuracy: 0.0143 - val_loss: 4.6149 - val_accuracy: 0.0100\n",
      "Epoch 9/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 7.5909 - accuracy: 0.0136 - val_loss: 4.6085 - val_accuracy: 0.0150\n",
      "Epoch 10/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 6.8747 - accuracy: 0.0136 - val_loss: 4.6077 - val_accuracy: 0.0150\n",
      "Epoch 11/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0542 - accuracy: 0.0100 - val_loss: 4.6062 - val_accuracy: 0.0167\n",
      "Epoch 12/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5983 - accuracy: 0.0186 - val_loss: 4.6049 - val_accuracy: 0.0167\n",
      "Epoch 13/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.2627 - accuracy: 0.0214 - val_loss: 4.6040 - val_accuracy: 0.0167\n",
      "Epoch 14/150\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 5.2875 - accuracy: 0.0207 - val_loss: 4.6032 - val_accuracy: 0.0167\n",
      "Epoch 15/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.0420 - accuracy: 0.0229 - val_loss: 4.6024 - val_accuracy: 0.0167\n",
      "Epoch 16/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 5.0080 - accuracy: 0.0164 - val_loss: 4.6016 - val_accuracy: 0.0167\n",
      "Epoch 17/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.9148 - accuracy: 0.0179 - val_loss: 4.6008 - val_accuracy: 0.0167\n",
      "Epoch 18/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.8441 - accuracy: 0.0200 - val_loss: 4.6000 - val_accuracy: 0.0167\n",
      "Epoch 19/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.8767 - accuracy: 0.0221 - val_loss: 4.5991 - val_accuracy: 0.0167\n",
      "Epoch 20/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.8445 - accuracy: 0.0200 - val_loss: 4.5984 - val_accuracy: 0.0167\n",
      "Epoch 21/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.8099 - accuracy: 0.0200 - val_loss: 4.5977 - val_accuracy: 0.0167\n",
      "Epoch 22/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.7427 - accuracy: 0.0264 - val_loss: 4.5969 - val_accuracy: 0.0283\n",
      "Epoch 23/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.8326 - accuracy: 0.0243 - val_loss: 4.5962 - val_accuracy: 0.0283\n",
      "Epoch 24/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.7113 - accuracy: 0.0243 - val_loss: 4.5955 - val_accuracy: 0.0283\n",
      "Epoch 25/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.8238 - accuracy: 0.0250 - val_loss: 4.5949 - val_accuracy: 0.0283\n",
      "Epoch 26/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.7125 - accuracy: 0.0279 - val_loss: 4.5942 - val_accuracy: 0.0283\n",
      "Epoch 27/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.7142 - accuracy: 0.0286 - val_loss: 4.5936 - val_accuracy: 0.0283\n",
      "Epoch 28/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.6930 - accuracy: 0.0229 - val_loss: 4.5930 - val_accuracy: 0.0283\n",
      "Epoch 29/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.7132 - accuracy: 0.0250 - val_loss: 4.5923 - val_accuracy: 0.0283\n",
      "Epoch 30/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.6929 - accuracy: 0.0264 - val_loss: 4.5917 - val_accuracy: 0.0283\n",
      "Epoch 31/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.6579 - accuracy: 0.0264 - val_loss: 4.5911 - val_accuracy: 0.0283\n",
      "Epoch 32/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.6667 - accuracy: 0.0257 - val_loss: 4.5906 - val_accuracy: 0.0283\n",
      "Epoch 33/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.7193 - accuracy: 0.0271 - val_loss: 4.5900 - val_accuracy: 0.0283\n",
      "Epoch 34/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.6330 - accuracy: 0.0279 - val_loss: 4.5895 - val_accuracy: 0.0283\n",
      "Epoch 35/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.6809 - accuracy: 0.0264 - val_loss: 4.5889 - val_accuracy: 0.0283\n",
      "Epoch 36/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.6060 - accuracy: 0.0271 - val_loss: 4.5885 - val_accuracy: 0.0283\n",
      "Epoch 37/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.6003 - accuracy: 0.0250 - val_loss: 4.5880 - val_accuracy: 0.0283\n",
      "Epoch 38/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.6348 - accuracy: 0.0264 - val_loss: 4.5875 - val_accuracy: 0.0283\n",
      "Epoch 39/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.6063 - accuracy: 0.0264 - val_loss: 4.5871 - val_accuracy: 0.0283\n",
      "Epoch 40/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6133 - accuracy: 0.0264 - val_loss: 4.5867 - val_accuracy: 0.0283\n",
      "Epoch 41/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6132 - accuracy: 0.0271 - val_loss: 4.5862 - val_accuracy: 0.0283\n",
      "Epoch 42/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6501 - accuracy: 0.0264 - val_loss: 4.5858 - val_accuracy: 0.0283\n",
      "Epoch 43/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.6069 - accuracy: 0.0264 - val_loss: 4.5854 - val_accuracy: 0.0283\n",
      "Epoch 44/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5936 - accuracy: 0.0257 - val_loss: 4.5850 - val_accuracy: 0.0283\n",
      "Epoch 45/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6018 - accuracy: 0.0264 - val_loss: 4.5846 - val_accuracy: 0.0283\n",
      "Epoch 46/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5942 - accuracy: 0.0257 - val_loss: 4.5843 - val_accuracy: 0.0283\n",
      "Epoch 47/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.5778 - accuracy: 0.0286 - val_loss: 4.5839 - val_accuracy: 0.0283\n",
      "Epoch 48/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.5936 - accuracy: 0.0271 - val_loss: 4.5836 - val_accuracy: 0.0283\n",
      "Epoch 49/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6142 - accuracy: 0.0250 - val_loss: 4.5832 - val_accuracy: 0.0283\n",
      "Epoch 50/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5841 - accuracy: 0.0264 - val_loss: 4.5829 - val_accuracy: 0.0283\n",
      "Epoch 51/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6276 - accuracy: 0.0271 - val_loss: 4.5826 - val_accuracy: 0.0283\n",
      "Epoch 52/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5786 - accuracy: 0.0271 - val_loss: 4.5824 - val_accuracy: 0.0283\n",
      "Epoch 53/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5605 - accuracy: 0.0271 - val_loss: 4.5821 - val_accuracy: 0.0283\n",
      "Epoch 54/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5841 - accuracy: 0.0264 - val_loss: 4.5818 - val_accuracy: 0.0283\n",
      "Epoch 55/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5736 - accuracy: 0.0279 - val_loss: 4.5815 - val_accuracy: 0.0283\n",
      "Epoch 56/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5559 - accuracy: 0.0293 - val_loss: 4.5813 - val_accuracy: 0.0283\n",
      "Epoch 57/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5597 - accuracy: 0.0279 - val_loss: 4.5811 - val_accuracy: 0.0283\n",
      "Epoch 58/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5932 - accuracy: 0.0286 - val_loss: 4.5808 - val_accuracy: 0.0283\n",
      "Epoch 59/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5704 - accuracy: 0.0271 - val_loss: 4.5806 - val_accuracy: 0.0283\n",
      "Epoch 60/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5840 - accuracy: 0.0279 - val_loss: 4.5804 - val_accuracy: 0.0283\n",
      "Epoch 61/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6194 - accuracy: 0.0257 - val_loss: 4.5802 - val_accuracy: 0.0283\n",
      "Epoch 62/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5570 - accuracy: 0.0271 - val_loss: 4.5800 - val_accuracy: 0.0283\n",
      "Epoch 63/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.5761 - accuracy: 0.0264 - val_loss: 4.5798 - val_accuracy: 0.0283\n",
      "Epoch 64/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5787 - accuracy: 0.0271 - val_loss: 4.5796 - val_accuracy: 0.0283\n",
      "Epoch 65/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5654 - accuracy: 0.0271 - val_loss: 4.5795 - val_accuracy: 0.0283\n",
      "Epoch 66/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5398 - accuracy: 0.0286 - val_loss: 4.5793 - val_accuracy: 0.0283\n",
      "Epoch 67/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.5665 - accuracy: 0.0271 - val_loss: 4.5792 - val_accuracy: 0.0283\n",
      "Epoch 68/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5591 - accuracy: 0.0271 - val_loss: 4.5791 - val_accuracy: 0.0283\n",
      "Epoch 69/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5625 - accuracy: 0.0271 - val_loss: 4.5789 - val_accuracy: 0.0283\n",
      "Epoch 70/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5624 - accuracy: 0.0264 - val_loss: 4.5788 - val_accuracy: 0.0283\n",
      "Epoch 71/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5741 - accuracy: 0.0271 - val_loss: 4.5787 - val_accuracy: 0.0283\n",
      "Epoch 72/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5445 - accuracy: 0.0271 - val_loss: 4.5785 - val_accuracy: 0.0283\n",
      "Epoch 73/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5535 - accuracy: 0.0264 - val_loss: 4.5785 - val_accuracy: 0.0283\n",
      "Epoch 74/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5587 - accuracy: 0.0264 - val_loss: 4.5784 - val_accuracy: 0.0283\n",
      "Epoch 75/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5418 - accuracy: 0.0271 - val_loss: 4.5783 - val_accuracy: 0.0283\n",
      "Epoch 76/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.5611 - accuracy: 0.0264 - val_loss: 4.5782 - val_accuracy: 0.0283\n",
      "Epoch 77/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.5376 - accuracy: 0.0264 - val_loss: 4.5782 - val_accuracy: 0.0283\n",
      "Epoch 78/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5462 - accuracy: 0.0264 - val_loss: 4.5781 - val_accuracy: 0.0283\n",
      "Epoch 79/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.5465 - accuracy: 0.0271 - val_loss: 4.5781 - val_accuracy: 0.0283\n",
      "Epoch 80/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.5436 - accuracy: 0.0271 - val_loss: 4.5780 - val_accuracy: 0.0283\n",
      "Epoch 81/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5574 - accuracy: 0.0271 - val_loss: 4.5779 - val_accuracy: 0.0283\n",
      "Epoch 82/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5460 - accuracy: 0.0271 - val_loss: 4.5779 - val_accuracy: 0.0283\n",
      "Epoch 83/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.5545 - accuracy: 0.0257 - val_loss: 4.5778 - val_accuracy: 0.0283\n",
      "Epoch 84/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.5330 - accuracy: 0.0271 - val_loss: 4.5778 - val_accuracy: 0.0283\n",
      "Epoch 85/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.5412 - accuracy: 0.0264 - val_loss: 4.5778 - val_accuracy: 0.0283\n",
      "Epoch 86/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5380 - accuracy: 0.0264 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 87/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5573 - accuracy: 0.0264 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 88/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5428 - accuracy: 0.0271 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 89/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.5402 - accuracy: 0.0264 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 90/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5524 - accuracy: 0.0271 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 91/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5436 - accuracy: 0.0264 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 92/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5516 - accuracy: 0.0271 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 93/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5359 - accuracy: 0.0271 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 94/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5367 - accuracy: 0.0271 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 95/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5275 - accuracy: 0.0264 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "Epoch 96/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.5282 - accuracy: 0.0271 - val_loss: 4.5777 - val_accuracy: 0.0283\n",
      "312/312 [==============================] - 0s 811us/step\n",
      "Epoch 1/150\n",
      "28/28 [==============================] - 2s 14ms/step - loss: 369.5910 - accuracy: 0.0103 - val_loss: 12.3875 - val_accuracy: 0.0080\n",
      "Epoch 2/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 37.1800 - accuracy: 0.0094 - val_loss: 4.6059 - val_accuracy: 0.0120\n",
      "Epoch 3/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 13.8326 - accuracy: 0.0126 - val_loss: 4.6031 - val_accuracy: 0.0127\n",
      "Epoch 4/150\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 8.1694 - accuracy: 0.0106 - val_loss: 4.6008 - val_accuracy: 0.0113\n",
      "Epoch 5/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 6.4451 - accuracy: 0.0129 - val_loss: 4.5982 - val_accuracy: 0.0120\n",
      "Epoch 6/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 5.6072 - accuracy: 0.0129 - val_loss: 4.5956 - val_accuracy: 0.0120\n",
      "Epoch 7/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 5.2269 - accuracy: 0.0194 - val_loss: 4.5931 - val_accuracy: 0.0287\n",
      "Epoch 8/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 5.0314 - accuracy: 0.0257 - val_loss: 4.5906 - val_accuracy: 0.0287\n",
      "Epoch 9/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.9301 - accuracy: 0.0249 - val_loss: 4.5883 - val_accuracy: 0.0287\n",
      "Epoch 10/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.9043 - accuracy: 0.0255 - val_loss: 4.5859 - val_accuracy: 0.0287\n",
      "Epoch 11/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.8124 - accuracy: 0.0240 - val_loss: 4.5838 - val_accuracy: 0.0287\n",
      "Epoch 12/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.7442 - accuracy: 0.0252 - val_loss: 4.5818 - val_accuracy: 0.0287\n",
      "Epoch 13/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.7012 - accuracy: 0.0277 - val_loss: 4.5798 - val_accuracy: 0.0287\n",
      "Epoch 14/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.6992 - accuracy: 0.0266 - val_loss: 4.5781 - val_accuracy: 0.0287\n",
      "Epoch 15/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.6903 - accuracy: 0.0269 - val_loss: 4.5762 - val_accuracy: 0.0287\n",
      "Epoch 16/150\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 4.6288 - accuracy: 0.0260 - val_loss: 4.5745 - val_accuracy: 0.0287\n",
      "Epoch 17/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.6381 - accuracy: 0.0257 - val_loss: 4.5727 - val_accuracy: 0.0287\n",
      "Epoch 18/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.6429 - accuracy: 0.0269 - val_loss: 4.5712 - val_accuracy: 0.0287\n",
      "Epoch 19/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.6066 - accuracy: 0.0266 - val_loss: 4.5698 - val_accuracy: 0.0287\n",
      "Epoch 20/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.6191 - accuracy: 0.0263 - val_loss: 4.5683 - val_accuracy: 0.0287\n",
      "Epoch 21/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.6148 - accuracy: 0.0263 - val_loss: 4.5670 - val_accuracy: 0.0287\n",
      "Epoch 22/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.6040 - accuracy: 0.0260 - val_loss: 4.5658 - val_accuracy: 0.0287\n",
      "Epoch 23/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5942 - accuracy: 0.0266 - val_loss: 4.5646 - val_accuracy: 0.0287\n",
      "Epoch 24/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5874 - accuracy: 0.0269 - val_loss: 4.5634 - val_accuracy: 0.0287\n",
      "Epoch 25/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.6079 - accuracy: 0.0269 - val_loss: 4.5623 - val_accuracy: 0.0287\n",
      "Epoch 26/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5902 - accuracy: 0.0266 - val_loss: 4.5614 - val_accuracy: 0.0287\n",
      "Epoch 27/150\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 4.5667 - accuracy: 0.0266 - val_loss: 4.5604 - val_accuracy: 0.0287\n",
      "Epoch 28/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5828 - accuracy: 0.0263 - val_loss: 4.5595 - val_accuracy: 0.0287\n",
      "Epoch 29/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5637 - accuracy: 0.0269 - val_loss: 4.5587 - val_accuracy: 0.0287\n",
      "Epoch 30/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5587 - accuracy: 0.0266 - val_loss: 4.5579 - val_accuracy: 0.0287\n",
      "Epoch 31/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5538 - accuracy: 0.0269 - val_loss: 4.5571 - val_accuracy: 0.0287\n",
      "Epoch 32/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5475 - accuracy: 0.0269 - val_loss: 4.5565 - val_accuracy: 0.0287\n",
      "Epoch 33/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5520 - accuracy: 0.0269 - val_loss: 4.5559 - val_accuracy: 0.0287\n",
      "Epoch 34/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5626 - accuracy: 0.0269 - val_loss: 4.5551 - val_accuracy: 0.0287\n",
      "Epoch 35/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5544 - accuracy: 0.0277 - val_loss: 4.5546 - val_accuracy: 0.0287\n",
      "Epoch 36/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5489 - accuracy: 0.0269 - val_loss: 4.5541 - val_accuracy: 0.0287\n",
      "Epoch 37/150\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 4.5400 - accuracy: 0.0263 - val_loss: 4.5536 - val_accuracy: 0.0287\n",
      "Epoch 38/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5488 - accuracy: 0.0266 - val_loss: 4.5532 - val_accuracy: 0.0287\n",
      "Epoch 39/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5409 - accuracy: 0.0269 - val_loss: 4.5528 - val_accuracy: 0.0287\n",
      "Epoch 40/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5439 - accuracy: 0.0266 - val_loss: 4.5525 - val_accuracy: 0.0287\n",
      "Epoch 41/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5495 - accuracy: 0.0269 - val_loss: 4.5521 - val_accuracy: 0.0287\n",
      "Epoch 42/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5633 - accuracy: 0.0269 - val_loss: 4.5517 - val_accuracy: 0.0287\n",
      "Epoch 43/150\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 4.5415 - accuracy: 0.0263 - val_loss: 4.5514 - val_accuracy: 0.0287\n",
      "Epoch 44/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5427 - accuracy: 0.0269 - val_loss: 4.5510 - val_accuracy: 0.0287\n",
      "Epoch 45/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5322 - accuracy: 0.0269 - val_loss: 4.5508 - val_accuracy: 0.0287\n",
      "Epoch 46/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5527 - accuracy: 0.0269 - val_loss: 4.5504 - val_accuracy: 0.0287\n",
      "Epoch 47/150\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 4.5396 - accuracy: 0.0269 - val_loss: 4.5504 - val_accuracy: 0.0287\n",
      "Epoch 48/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5505 - accuracy: 0.0269 - val_loss: 4.5501 - val_accuracy: 0.0287\n",
      "Epoch 49/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5373 - accuracy: 0.0269 - val_loss: 4.5499 - val_accuracy: 0.0287\n",
      "Epoch 50/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5408 - accuracy: 0.0269 - val_loss: 4.5497 - val_accuracy: 0.0287\n",
      "Epoch 51/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5366 - accuracy: 0.0263 - val_loss: 4.5495 - val_accuracy: 0.0287\n",
      "Epoch 52/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5389 - accuracy: 0.0269 - val_loss: 4.5494 - val_accuracy: 0.0287\n",
      "Epoch 53/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5300 - accuracy: 0.0266 - val_loss: 4.5493 - val_accuracy: 0.0287\n",
      "Epoch 54/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5263 - accuracy: 0.0266 - val_loss: 4.5491 - val_accuracy: 0.0287\n",
      "Epoch 55/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5297 - accuracy: 0.0269 - val_loss: 4.5490 - val_accuracy: 0.0287\n",
      "Epoch 56/150\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 4.5386 - accuracy: 0.0269 - val_loss: 4.5490 - val_accuracy: 0.0287\n",
      "Epoch 57/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5269 - accuracy: 0.0263 - val_loss: 4.5489 - val_accuracy: 0.0287\n",
      "Epoch 58/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5415 - accuracy: 0.0269 - val_loss: 4.5487 - val_accuracy: 0.0287\n",
      "Epoch 59/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5384 - accuracy: 0.0269 - val_loss: 4.5486 - val_accuracy: 0.0287\n",
      "Epoch 60/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5328 - accuracy: 0.0269 - val_loss: 4.5484 - val_accuracy: 0.0287\n",
      "Epoch 61/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5270 - accuracy: 0.0269 - val_loss: 4.5484 - val_accuracy: 0.0287\n",
      "Epoch 62/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5291 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 63/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5284 - accuracy: 0.0266 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 64/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5276 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 65/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5285 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 66/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5279 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 67/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5285 - accuracy: 0.0269 - val_loss: 4.5482 - val_accuracy: 0.0287\n",
      "Epoch 68/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5285 - accuracy: 0.0269 - val_loss: 4.5482 - val_accuracy: 0.0287\n",
      "Epoch 69/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5233 - accuracy: 0.0269 - val_loss: 4.5482 - val_accuracy: 0.0287\n",
      "Epoch 70/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5249 - accuracy: 0.0269 - val_loss: 4.5484 - val_accuracy: 0.0287\n",
      "Epoch 71/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5234 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 72/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5333 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 73/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5249 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 74/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5220 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 75/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5244 - accuracy: 0.0272 - val_loss: 4.5482 - val_accuracy: 0.0287\n",
      "Epoch 76/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5432 - accuracy: 0.0266 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 77/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5239 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 78/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5229 - accuracy: 0.0269 - val_loss: 4.5484 - val_accuracy: 0.0287\n",
      "Epoch 79/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5219 - accuracy: 0.0269 - val_loss: 4.5483 - val_accuracy: 0.0287\n",
      "Epoch 80/150\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4.5248 - accuracy: 0.0269 - val_loss: 4.5482 - val_accuracy: 0.0287\n",
      "Epoch 81/150\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4.5254 - accuracy: 0.0269 - val_loss: 4.5484 - val_accuracy: 0.0287\n",
      "Epoch 82/150\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4.5250 - accuracy: 0.0269 - val_loss: 4.5484 - val_accuracy: 0.0287\n",
      "312/312 [==============================] - 0s 807us/step\n",
      "Epoch 1/150\n",
      "44/44 [==============================] - 2s 11ms/step - loss: 142.1524 - accuracy: 0.0116 - val_loss: 5.9605 - val_accuracy: 0.0092\n",
      "Epoch 2/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 13.0696 - accuracy: 0.0102 - val_loss: 4.6057 - val_accuracy: 0.0188\n",
      "Epoch 3/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 6.1247 - accuracy: 0.0171 - val_loss: 4.6011 - val_accuracy: 0.0188\n",
      "Epoch 4/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.2749 - accuracy: 0.0193 - val_loss: 4.5965 - val_accuracy: 0.0188\n",
      "Epoch 5/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8900 - accuracy: 0.0211 - val_loss: 4.5921 - val_accuracy: 0.0304\n",
      "Epoch 6/150\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8022 - accuracy: 0.0261 - val_loss: 4.5879 - val_accuracy: 0.0304\n",
      "Epoch 7/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.7091 - accuracy: 0.0259 - val_loss: 4.5840 - val_accuracy: 0.0304\n",
      "Epoch 8/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.6908 - accuracy: 0.0254 - val_loss: 4.5804 - val_accuracy: 0.0304\n",
      "Epoch 9/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.6697 - accuracy: 0.0259 - val_loss: 4.5770 - val_accuracy: 0.0304\n",
      "Epoch 10/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.6320 - accuracy: 0.0259 - val_loss: 4.5740 - val_accuracy: 0.0304\n",
      "Epoch 11/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.6243 - accuracy: 0.0262 - val_loss: 4.5710 - val_accuracy: 0.0304\n",
      "Epoch 12/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.6117 - accuracy: 0.0255 - val_loss: 4.5685 - val_accuracy: 0.0304\n",
      "Epoch 13/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.6142 - accuracy: 0.0262 - val_loss: 4.5660 - val_accuracy: 0.0304\n",
      "Epoch 14/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5974 - accuracy: 0.0259 - val_loss: 4.5637 - val_accuracy: 0.0304\n",
      "Epoch 15/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5837 - accuracy: 0.0257 - val_loss: 4.5617 - val_accuracy: 0.0304\n",
      "Epoch 16/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5766 - accuracy: 0.0255 - val_loss: 4.5600 - val_accuracy: 0.0304\n",
      "Epoch 17/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5679 - accuracy: 0.0257 - val_loss: 4.5583 - val_accuracy: 0.0304\n",
      "Epoch 18/150\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 4.5607 - accuracy: 0.0259 - val_loss: 4.5566 - val_accuracy: 0.0304\n",
      "Epoch 19/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5669 - accuracy: 0.0261 - val_loss: 4.5553 - val_accuracy: 0.0304\n",
      "Epoch 20/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5556 - accuracy: 0.0261 - val_loss: 4.5539 - val_accuracy: 0.0304\n",
      "Epoch 21/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5625 - accuracy: 0.0261 - val_loss: 4.5528 - val_accuracy: 0.0304\n",
      "Epoch 22/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5618 - accuracy: 0.0266 - val_loss: 4.5518 - val_accuracy: 0.0304\n",
      "Epoch 23/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5466 - accuracy: 0.0261 - val_loss: 4.5508 - val_accuracy: 0.0304\n",
      "Epoch 24/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5568 - accuracy: 0.0261 - val_loss: 4.5499 - val_accuracy: 0.0304\n",
      "Epoch 25/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5446 - accuracy: 0.0259 - val_loss: 4.5492 - val_accuracy: 0.0304\n",
      "Epoch 26/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5481 - accuracy: 0.0259 - val_loss: 4.5485 - val_accuracy: 0.0304\n",
      "Epoch 27/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5408 - accuracy: 0.0261 - val_loss: 4.5478 - val_accuracy: 0.0304\n",
      "Epoch 28/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5479 - accuracy: 0.0261 - val_loss: 4.5473 - val_accuracy: 0.0304\n",
      "Epoch 29/150\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 4.5410 - accuracy: 0.0261 - val_loss: 4.5469 - val_accuracy: 0.0304\n",
      "Epoch 30/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5366 - accuracy: 0.0261 - val_loss: 4.5464 - val_accuracy: 0.0304\n",
      "Epoch 31/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5382 - accuracy: 0.0261 - val_loss: 4.5459 - val_accuracy: 0.0304\n",
      "Epoch 32/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5348 - accuracy: 0.0261 - val_loss: 4.5457 - val_accuracy: 0.0304\n",
      "Epoch 33/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5345 - accuracy: 0.0261 - val_loss: 4.5453 - val_accuracy: 0.0304\n",
      "Epoch 34/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5351 - accuracy: 0.0261 - val_loss: 4.5451 - val_accuracy: 0.0304\n",
      "Epoch 35/150\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.5321 - accuracy: 0.0262 - val_loss: 4.5449 - val_accuracy: 0.0304\n",
      "Epoch 36/150\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 4.5345 - accuracy: 0.0261 - val_loss: 4.5446 - val_accuracy: 0.0304\n",
      "Epoch 37/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5317 - accuracy: 0.0262 - val_loss: 4.5445 - val_accuracy: 0.0304\n",
      "Epoch 38/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5330 - accuracy: 0.0261 - val_loss: 4.5444 - val_accuracy: 0.0304\n",
      "Epoch 39/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5312 - accuracy: 0.0262 - val_loss: 4.5443 - val_accuracy: 0.0304\n",
      "Epoch 40/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5350 - accuracy: 0.0261 - val_loss: 4.5441 - val_accuracy: 0.0304\n",
      "Epoch 41/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5260 - accuracy: 0.0261 - val_loss: 4.5440 - val_accuracy: 0.0304\n",
      "Epoch 42/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5233 - accuracy: 0.0262 - val_loss: 4.5440 - val_accuracy: 0.0304\n",
      "Epoch 43/150\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.5299 - accuracy: 0.0261 - val_loss: 4.5439 - val_accuracy: 0.0304\n",
      "Epoch 44/150\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.5246 - accuracy: 0.0261 - val_loss: 4.5439 - val_accuracy: 0.0304\n",
      "Epoch 45/150\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.5278 - accuracy: 0.0261 - val_loss: 4.5438 - val_accuracy: 0.0304\n",
      "Epoch 46/150\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 4.5235 - accuracy: 0.0261 - val_loss: 4.5438 - val_accuracy: 0.0304\n",
      "Epoch 47/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5269 - accuracy: 0.0261 - val_loss: 4.5438 - val_accuracy: 0.0304\n",
      "Epoch 48/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5294 - accuracy: 0.0261 - val_loss: 4.5438 - val_accuracy: 0.0304\n",
      "Epoch 49/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5225 - accuracy: 0.0261 - val_loss: 4.5438 - val_accuracy: 0.0304\n",
      "Epoch 50/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5234 - accuracy: 0.0261 - val_loss: 4.5438 - val_accuracy: 0.0304\n",
      "Epoch 51/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5217 - accuracy: 0.0261 - val_loss: 4.5439 - val_accuracy: 0.0304\n",
      "Epoch 52/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5236 - accuracy: 0.0261 - val_loss: 4.5439 - val_accuracy: 0.0304\n",
      "Epoch 53/150\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5215 - accuracy: 0.0261 - val_loss: 4.5439 - val_accuracy: 0.0304\n",
      "Epoch 54/150\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 4.5217 - accuracy: 0.0261 - val_loss: 4.5439 - val_accuracy: 0.0304\n",
      "312/312 [==============================] - 0s 901us/step\n",
      "Epoch 1/150\n",
      "55/55 [==============================] - 2s 10ms/step - loss: 98.3090 - accuracy: 0.0119 - val_loss: 4.7533 - val_accuracy: 0.0113\n",
      "Epoch 2/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 10.7470 - accuracy: 0.0106 - val_loss: 4.6069 - val_accuracy: 0.0087\n",
      "Epoch 3/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 5.8952 - accuracy: 0.0089 - val_loss: 4.6015 - val_accuracy: 0.0087\n",
      "Epoch 4/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 5.0567 - accuracy: 0.0154 - val_loss: 4.5956 - val_accuracy: 0.0130\n",
      "Epoch 5/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.8065 - accuracy: 0.0224 - val_loss: 4.5900 - val_accuracy: 0.0287\n",
      "Epoch 6/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.7017 - accuracy: 0.0256 - val_loss: 4.5850 - val_accuracy: 0.0287\n",
      "Epoch 7/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.6701 - accuracy: 0.0258 - val_loss: 4.5804 - val_accuracy: 0.0287\n",
      "Epoch 8/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.6256 - accuracy: 0.0261 - val_loss: 4.5761 - val_accuracy: 0.0287\n",
      "Epoch 9/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.6179 - accuracy: 0.0260 - val_loss: 4.5724 - val_accuracy: 0.0287\n",
      "Epoch 10/150\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 4.5937 - accuracy: 0.0261 - val_loss: 4.5689 - val_accuracy: 0.0287\n",
      "Epoch 11/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5781 - accuracy: 0.0268 - val_loss: 4.5660 - val_accuracy: 0.0287\n",
      "Epoch 12/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5704 - accuracy: 0.0266 - val_loss: 4.5632 - val_accuracy: 0.0287\n",
      "Epoch 13/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5729 - accuracy: 0.0268 - val_loss: 4.5609 - val_accuracy: 0.0287\n",
      "Epoch 14/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5586 - accuracy: 0.0270 - val_loss: 4.5587 - val_accuracy: 0.0287\n",
      "Epoch 15/150\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 4.5574 - accuracy: 0.0266 - val_loss: 4.5568 - val_accuracy: 0.0287\n",
      "Epoch 16/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5584 - accuracy: 0.0266 - val_loss: 4.5550 - val_accuracy: 0.0287\n",
      "Epoch 17/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5485 - accuracy: 0.0266 - val_loss: 4.5537 - val_accuracy: 0.0287\n",
      "Epoch 18/150\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 4.5493 - accuracy: 0.0266 - val_loss: 4.5523 - val_accuracy: 0.0287\n",
      "Epoch 19/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5467 - accuracy: 0.0266 - val_loss: 4.5511 - val_accuracy: 0.0287\n",
      "Epoch 20/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5437 - accuracy: 0.0264 - val_loss: 4.5501 - val_accuracy: 0.0287\n",
      "Epoch 21/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5363 - accuracy: 0.0268 - val_loss: 4.5491 - val_accuracy: 0.0287\n",
      "Epoch 22/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5381 - accuracy: 0.0266 - val_loss: 4.5485 - val_accuracy: 0.0287\n",
      "Epoch 23/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5364 - accuracy: 0.0267 - val_loss: 4.5477 - val_accuracy: 0.0287\n",
      "Epoch 24/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5365 - accuracy: 0.0266 - val_loss: 4.5471 - val_accuracy: 0.0287\n",
      "Epoch 25/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5321 - accuracy: 0.0266 - val_loss: 4.5467 - val_accuracy: 0.0287\n",
      "Epoch 26/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5299 - accuracy: 0.0267 - val_loss: 4.5462 - val_accuracy: 0.0287\n",
      "Epoch 27/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5341 - accuracy: 0.0266 - val_loss: 4.5459 - val_accuracy: 0.0287\n",
      "Epoch 28/150\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 4.5283 - accuracy: 0.0266 - val_loss: 4.5456 - val_accuracy: 0.0287\n",
      "Epoch 29/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5295 - accuracy: 0.0267 - val_loss: 4.5453 - val_accuracy: 0.0287\n",
      "Epoch 30/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5269 - accuracy: 0.0267 - val_loss: 4.5451 - val_accuracy: 0.0287\n",
      "Epoch 31/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5257 - accuracy: 0.0268 - val_loss: 4.5450 - val_accuracy: 0.0287\n",
      "Epoch 32/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5280 - accuracy: 0.0267 - val_loss: 4.5448 - val_accuracy: 0.0287\n",
      "Epoch 33/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5267 - accuracy: 0.0268 - val_loss: 4.5447 - val_accuracy: 0.0287\n",
      "Epoch 34/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5257 - accuracy: 0.0266 - val_loss: 4.5446 - val_accuracy: 0.0287\n",
      "Epoch 35/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5230 - accuracy: 0.0268 - val_loss: 4.5446 - val_accuracy: 0.0287\n",
      "Epoch 36/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5232 - accuracy: 0.0267 - val_loss: 4.5446 - val_accuracy: 0.0287\n",
      "Epoch 37/150\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 4.5233 - accuracy: 0.0268 - val_loss: 4.5445 - val_accuracy: 0.0287\n",
      "Epoch 38/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5227 - accuracy: 0.0267 - val_loss: 4.5445 - val_accuracy: 0.0287\n",
      "Epoch 39/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5235 - accuracy: 0.0267 - val_loss: 4.5444 - val_accuracy: 0.0287\n",
      "Epoch 40/150\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 4.5259 - accuracy: 0.0267 - val_loss: 4.5445 - val_accuracy: 0.0287\n",
      "Epoch 41/150\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 4.5240 - accuracy: 0.0267 - val_loss: 4.5445 - val_accuracy: 0.0287\n",
      "Epoch 42/150\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 4.5224 - accuracy: 0.0267 - val_loss: 4.5445 - val_accuracy: 0.0287\n",
      "Epoch 43/150\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 4.5222 - accuracy: 0.0267 - val_loss: 4.5446 - val_accuracy: 0.0287\n",
      "Epoch 44/150\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 4.5227 - accuracy: 0.0268 - val_loss: 4.5446 - val_accuracy: 0.0287\n",
      "Epoch 45/150\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 4.5233 - accuracy: 0.0267 - val_loss: 4.5447 - val_accuracy: 0.0287\n",
      "Epoch 46/150\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 4.5214 - accuracy: 0.0267 - val_loss: 4.5447 - val_accuracy: 0.0287\n",
      "312/312 [==============================] - 0s 1ms/step\n",
      "Epoch 1/150\n",
      "83/83 [==============================] - 2s 9ms/step - loss: 87.0748 - accuracy: 0.0127 - val_loss: 4.6077 - val_accuracy: 0.0127\n",
      "Epoch 2/150\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 6.6622 - accuracy: 0.0106 - val_loss: 4.5982 - val_accuracy: 0.0173\n",
      "Epoch 3/150\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 4.8928 - accuracy: 0.0165 - val_loss: 4.5894 - val_accuracy: 0.0176\n",
      "Epoch 4/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.6976 - accuracy: 0.0186 - val_loss: 4.5817 - val_accuracy: 0.0176\n",
      "Epoch 5/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.6368 - accuracy: 0.0242 - val_loss: 4.5749 - val_accuracy: 0.0289\n",
      "Epoch 6/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5991 - accuracy: 0.0265 - val_loss: 4.5689 - val_accuracy: 0.0289\n",
      "Epoch 7/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5909 - accuracy: 0.0264 - val_loss: 4.5638 - val_accuracy: 0.0289\n",
      "Epoch 8/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5768 - accuracy: 0.0265 - val_loss: 4.5593 - val_accuracy: 0.0289\n",
      "Epoch 9/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5678 - accuracy: 0.0261 - val_loss: 4.5553 - val_accuracy: 0.0289\n",
      "Epoch 10/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5632 - accuracy: 0.0264 - val_loss: 4.5520 - val_accuracy: 0.0289\n",
      "Epoch 11/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5599 - accuracy: 0.0268 - val_loss: 4.5492 - val_accuracy: 0.0289\n",
      "Epoch 12/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5480 - accuracy: 0.0264 - val_loss: 4.5468 - val_accuracy: 0.0289\n",
      "Epoch 13/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5505 - accuracy: 0.0269 - val_loss: 4.5448 - val_accuracy: 0.0289\n",
      "Epoch 14/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5453 - accuracy: 0.0267 - val_loss: 4.5431 - val_accuracy: 0.0289\n",
      "Epoch 15/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5447 - accuracy: 0.0267 - val_loss: 4.5417 - val_accuracy: 0.0289\n",
      "Epoch 16/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5421 - accuracy: 0.0265 - val_loss: 4.5404 - val_accuracy: 0.0289\n",
      "Epoch 17/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5362 - accuracy: 0.0268 - val_loss: 4.5394 - val_accuracy: 0.0289\n",
      "Epoch 18/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5372 - accuracy: 0.0266 - val_loss: 4.5385 - val_accuracy: 0.0289\n",
      "Epoch 19/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5342 - accuracy: 0.0266 - val_loss: 4.5378 - val_accuracy: 0.0289\n",
      "Epoch 20/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5321 - accuracy: 0.0267 - val_loss: 4.5371 - val_accuracy: 0.0289\n",
      "Epoch 21/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5295 - accuracy: 0.0267 - val_loss: 4.5366 - val_accuracy: 0.0289\n",
      "Epoch 22/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5284 - accuracy: 0.0267 - val_loss: 4.5362 - val_accuracy: 0.0289\n",
      "Epoch 23/150\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 4.5303 - accuracy: 0.0266 - val_loss: 4.5357 - val_accuracy: 0.0289\n",
      "Epoch 24/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5274 - accuracy: 0.0266 - val_loss: 4.5354 - val_accuracy: 0.0289\n",
      "Epoch 25/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5273 - accuracy: 0.0267 - val_loss: 4.5351 - val_accuracy: 0.0289\n",
      "Epoch 26/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5281 - accuracy: 0.0268 - val_loss: 4.5350 - val_accuracy: 0.0289\n",
      "Epoch 27/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5259 - accuracy: 0.0270 - val_loss: 4.5347 - val_accuracy: 0.0289\n",
      "Epoch 28/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5275 - accuracy: 0.0267 - val_loss: 4.5346 - val_accuracy: 0.0289\n",
      "Epoch 29/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5258 - accuracy: 0.0269 - val_loss: 4.5345 - val_accuracy: 0.0289\n",
      "Epoch 30/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5246 - accuracy: 0.0266 - val_loss: 4.5345 - val_accuracy: 0.0289\n",
      "Epoch 31/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5254 - accuracy: 0.0266 - val_loss: 4.5345 - val_accuracy: 0.0289\n",
      "Epoch 32/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5262 - accuracy: 0.0266 - val_loss: 4.5345 - val_accuracy: 0.0289\n",
      "Epoch 33/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5246 - accuracy: 0.0267 - val_loss: 4.5344 - val_accuracy: 0.0289\n",
      "Epoch 34/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5246 - accuracy: 0.0267 - val_loss: 4.5342 - val_accuracy: 0.0289\n",
      "Epoch 35/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5239 - accuracy: 0.0267 - val_loss: 4.5342 - val_accuracy: 0.0289\n",
      "Epoch 36/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5238 - accuracy: 0.0267 - val_loss: 4.5341 - val_accuracy: 0.0289\n",
      "Epoch 37/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5246 - accuracy: 0.0268 - val_loss: 4.5340 - val_accuracy: 0.0289\n",
      "Epoch 38/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5235 - accuracy: 0.0267 - val_loss: 4.5339 - val_accuracy: 0.0289\n",
      "Epoch 39/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5236 - accuracy: 0.0268 - val_loss: 4.5339 - val_accuracy: 0.0289\n",
      "Epoch 40/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5235 - accuracy: 0.0267 - val_loss: 4.5339 - val_accuracy: 0.0289\n",
      "Epoch 41/150\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 4.5245 - accuracy: 0.0266 - val_loss: 4.5339 - val_accuracy: 0.0289\n",
      "Epoch 42/150\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 4.5233 - accuracy: 0.0265 - val_loss: 4.5339 - val_accuracy: 0.0289\n",
      "Epoch 43/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5248 - accuracy: 0.0267 - val_loss: 4.5340 - val_accuracy: 0.0289\n",
      "Epoch 44/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5238 - accuracy: 0.0267 - val_loss: 4.5339 - val_accuracy: 0.0289\n",
      "Epoch 45/150\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 4.5238 - accuracy: 0.0267 - val_loss: 4.5339 - val_accuracy: 0.0289\n",
      "312/312 [==============================] - 0s 992us/step\n",
      "Epoch 1/150\n",
      "110/110 [==============================] - 2s 7ms/step - loss: 68.4834 - accuracy: 0.0114 - val_loss: 4.6041 - val_accuracy: 0.0115\n",
      "Epoch 2/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 5.7272 - accuracy: 0.0196 - val_loss: 4.5920 - val_accuracy: 0.0275\n",
      "Epoch 3/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.8052 - accuracy: 0.0254 - val_loss: 4.5801 - val_accuracy: 0.0275\n",
      "Epoch 4/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.7159 - accuracy: 0.0267 - val_loss: 4.5701 - val_accuracy: 0.0275\n",
      "Epoch 5/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.6162 - accuracy: 0.0266 - val_loss: 4.5617 - val_accuracy: 0.0275\n",
      "Epoch 6/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.6190 - accuracy: 0.0269 - val_loss: 4.5548 - val_accuracy: 0.0275\n",
      "Epoch 7/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5891 - accuracy: 0.0269 - val_loss: 4.5491 - val_accuracy: 0.0275\n",
      "Epoch 8/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5789 - accuracy: 0.0266 - val_loss: 4.5446 - val_accuracy: 0.0275\n",
      "Epoch 9/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5660 - accuracy: 0.0270 - val_loss: 4.5407 - val_accuracy: 0.0275\n",
      "Epoch 10/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5593 - accuracy: 0.0272 - val_loss: 4.5376 - val_accuracy: 0.0275\n",
      "Epoch 11/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5523 - accuracy: 0.0270 - val_loss: 4.5350 - val_accuracy: 0.0275\n",
      "Epoch 12/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5475 - accuracy: 0.0274 - val_loss: 4.5329 - val_accuracy: 0.0275\n",
      "Epoch 13/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5423 - accuracy: 0.0271 - val_loss: 4.5313 - val_accuracy: 0.0275\n",
      "Epoch 14/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5388 - accuracy: 0.0273 - val_loss: 4.5299 - val_accuracy: 0.0275\n",
      "Epoch 15/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5398 - accuracy: 0.0273 - val_loss: 4.5288 - val_accuracy: 0.0275\n",
      "Epoch 16/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5389 - accuracy: 0.0272 - val_loss: 4.5279 - val_accuracy: 0.0275\n",
      "Epoch 17/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5355 - accuracy: 0.0273 - val_loss: 4.5271 - val_accuracy: 0.0275\n",
      "Epoch 18/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5433 - accuracy: 0.0272 - val_loss: 4.5265 - val_accuracy: 0.0275\n",
      "Epoch 19/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5334 - accuracy: 0.0274 - val_loss: 4.5260 - val_accuracy: 0.0275\n",
      "Epoch 20/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5373 - accuracy: 0.0272 - val_loss: 4.5256 - val_accuracy: 0.0275\n",
      "Epoch 21/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5326 - accuracy: 0.0273 - val_loss: 4.5252 - val_accuracy: 0.0275\n",
      "Epoch 22/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5304 - accuracy: 0.0273 - val_loss: 4.5250 - val_accuracy: 0.0275\n",
      "Epoch 23/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5318 - accuracy: 0.0272 - val_loss: 4.5248 - val_accuracy: 0.0275\n",
      "Epoch 24/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5317 - accuracy: 0.0273 - val_loss: 4.5246 - val_accuracy: 0.0275\n",
      "Epoch 25/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5339 - accuracy: 0.0272 - val_loss: 4.5244 - val_accuracy: 0.0275\n",
      "Epoch 26/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5302 - accuracy: 0.0273 - val_loss: 4.5243 - val_accuracy: 0.0275\n",
      "Epoch 27/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5300 - accuracy: 0.0273 - val_loss: 4.5241 - val_accuracy: 0.0275\n",
      "Epoch 28/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5285 - accuracy: 0.0273 - val_loss: 4.5240 - val_accuracy: 0.0275\n",
      "Epoch 29/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5293 - accuracy: 0.0272 - val_loss: 4.5239 - val_accuracy: 0.0275\n",
      "Epoch 30/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5290 - accuracy: 0.0273 - val_loss: 4.5238 - val_accuracy: 0.0275\n",
      "Epoch 31/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5297 - accuracy: 0.0271 - val_loss: 4.5238 - val_accuracy: 0.0275\n",
      "Epoch 32/150\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 4.5283 - accuracy: 0.0273 - val_loss: 4.5237 - val_accuracy: 0.0275\n",
      "Epoch 33/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5286 - accuracy: 0.0273 - val_loss: 4.5237 - val_accuracy: 0.0275\n",
      "Epoch 34/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5282 - accuracy: 0.0273 - val_loss: 4.5237 - val_accuracy: 0.0275\n",
      "Epoch 35/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5298 - accuracy: 0.0273 - val_loss: 4.5236 - val_accuracy: 0.0275\n",
      "Epoch 36/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5290 - accuracy: 0.0273 - val_loss: 4.5236 - val_accuracy: 0.0275\n",
      "Epoch 37/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5277 - accuracy: 0.0272 - val_loss: 4.5235 - val_accuracy: 0.0275\n",
      "Epoch 38/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5287 - accuracy: 0.0273 - val_loss: 4.5235 - val_accuracy: 0.0275\n",
      "Epoch 39/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5282 - accuracy: 0.0272 - val_loss: 4.5235 - val_accuracy: 0.0275\n",
      "Epoch 40/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5281 - accuracy: 0.0274 - val_loss: 4.5235 - val_accuracy: 0.0275\n",
      "Epoch 41/150\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 4.5286 - accuracy: 0.0273 - val_loss: 4.5234 - val_accuracy: 0.0275\n",
      "Epoch 42/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5272 - accuracy: 0.0272 - val_loss: 4.5234 - val_accuracy: 0.0275\n",
      "Epoch 43/150\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 4.5288 - accuracy: 0.0273 - val_loss: 4.5234 - val_accuracy: 0.0275\n",
      "Epoch 44/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5272 - accuracy: 0.0272 - val_loss: 4.5233 - val_accuracy: 0.0275\n",
      "Epoch 45/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5286 - accuracy: 0.0273 - val_loss: 4.5233 - val_accuracy: 0.0275\n",
      "Epoch 46/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5305 - accuracy: 0.0273 - val_loss: 4.5234 - val_accuracy: 0.0275\n",
      "Epoch 47/150\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 4.5270 - accuracy: 0.0273 - val_loss: 4.5232 - val_accuracy: 0.0275\n",
      "Epoch 48/150\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 4.5272 - accuracy: 0.0273 - val_loss: 4.5232 - val_accuracy: 0.0275\n",
      "Epoch 49/150\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 4.5269 - accuracy: 0.0274"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26712/3641395655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_nn_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     h = model.fit(X_train, y_train, batch_size=128, epochs=150, validation_data=(X_val, y_val), \n\u001b[0m\u001b[0;32m      6\u001b[0m         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True,start_from_epoch=30)])\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for sample_size in [1000,2000,5000,8000,10000,15000,20000,30000]:\n",
    "    X_train,X_val,y_train,y_val = load_nn_data(sample_size)\n",
    "    model = make_model(params)\n",
    "    h = model.fit(X_train, y_train, batch_size=128, epochs=150, validation_data=(X_val, y_val), \n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True,start_from_epoch=30)])\n",
    "    \n",
    "    history  = {\n",
    "        'val_loss':h.history['val_loss'],\n",
    "        'val_accuracy':h.history['val_accuracy'],\n",
    "        'loss':h.history['loss'],\n",
    "        'accuracy':h.history['accuracy'],\n",
    "        'sample_size' : sample_size,\n",
    "    }\n",
    "    \n",
    "    res = test_nn_model(model)\n",
    "    res['sample_size'] = sample_size\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018855</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy_score  balanced_accuracy_score  recall_score  f1_score  \\\n",
       "0        0.018855                     0.01          0.01  0.000370   \n",
       "1        0.028683                     0.01          0.01  0.000558   \n",
       "2        0.028683                     0.01          0.01  0.000558   \n",
       "3        0.028683                     0.01          0.01  0.000558   \n",
       "4        0.028683                     0.01          0.01  0.000558   \n",
       "5        0.028683                     0.01          0.01  0.000558   \n",
       "\n",
       "   sample_size  \n",
       "0         1000  \n",
       "1         2000  \n",
       "2         5000  \n",
       "3         8000  \n",
       "4        10000  \n",
       "5        15000  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b7bbe84c70>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(history[6].history['loss'], label=\"train_loss\")\n",
    "plt.plot(history[6].history['val_loss'], label=\"val_loss\")\n",
    "\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
